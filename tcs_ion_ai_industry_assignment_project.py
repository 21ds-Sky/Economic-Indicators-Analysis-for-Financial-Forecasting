# -*- coding: utf-8 -*-
"""TCS-ion AI industry assignment project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X7QVdEl7QvVAF19jZOf5d0DR5TIE1mUQ
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from xgboost import XGBRegressor

# Step 1: Data Preparation
# Load the dataset
data = pd.read_csv('gdpWorld.csv')

print(data.columns)

data.columns = data.columns.str.strip()
print(data.columns)

import pandas as pd

# Load your dataset (replace '/content/gdpWorld.csv' with the actual path to your dataset file)
df = pd.read_csv('/content/gdpWorld.csv')

# Display the first few rows of the dataset
print("Head of the dataset:")
print(df.head())

# Get summary information about the dataset
print("\nInfo about the dataset:")
print(df.info())

# Generate descriptive statistics for numerical columns
print("\nSummary statistics for numerical columns:")
print(df.describe())

# Check for missing values in each column
print("\nMissing values per column:")
print(df.isnull().sum())

# Select 10 factors affecting GDP (e.g., population, literacy, birth-rate, deathrate, arable, industry)
selected_factors = ['Population', 'Literacy (%)', 'Arable (%)', 'Birthrate', 'Deathrate','GDP ($ per capita)', 'Industry', 'Service', 'Pop. Density (per sq. mi.)', 'Net migration', 'Infant mortality (per 1000 births)']

# Step 2: Data Preprocessing
# Handle missing values
data.dropna(subset=selected_factors, inplace=True)

# Encoding Categorical Variables
import pandas as pd

# Assuming you have a DataFrame named 'data'
categorical_columns = ['Population', 'Pop. Density (per sq. mi.)', 'Literacy (%)', 'Arable (%)', 'Birthrate', 'Deathrate','GDP ($ per capita)', 'Industry', 'Service', 'Net migration', 'Infant mortality (per 1000 births)']

# Iterate through the specified columns and preprocess the data
for column in categorical_columns:
    data[column] = data[column].astype(str).str.replace(',', '.').astype(float)

# Now your data should have the correct numeric format

# Split dataset into features (X) and target variable (y)
X = data[selected_factors[:-1]]
y = data[selected_factors[-1]]

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 3: Model Building
# Create and train a linear regression model
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)

# Create and train a non-linear model (e.g., XGBoost)
non_linear_model = XGBRegressor()
non_linear_model.fit(X_train, y_train)

# Step 4: Model Evaluation
# Predict on the testing data
linear_predictions = linear_model.predict(X_test)
non_linear_predictions = non_linear_model.predict(X_test)

# Calculate evaluation metrics
linear_mae = mean_absolute_error(y_test, linear_predictions)
non_linear_mae = mean_absolute_error(y_test, non_linear_predictions)

linear_mse = mean_squared_error(y_test, linear_predictions)
non_linear_mse = mean_squared_error(y_test, non_linear_predictions)

linear_r2 = r2_score(y_test, linear_predictions)
non_linear_r2 = r2_score(y_test, non_linear_predictions)

# Step 5: Compare Models
print("Linear Model MAE:", linear_mae)
print("Non-linear Model MAE:", non_linear_mae)

print("Linear Model MSE:", linear_mse)
print("Non-linear Model MSE:", non_linear_mse)

print("Linear Model R2 Score:", linear_r2)
print("Non-linear Model R2 Score:", non_linear_r2)

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression


# Choose and initialize your machine learning model (e.g., Non-Linear Regression)
best_model = XGBRegressor()

# Train the model on the training data
best_model.fit(X_train, y_train)

# Make predictions on the testing data
test_predictions = best_model.predict(X_test)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Make predictions on the testing dataset
test_predictions = best_model.predict(X_test)

# Calculate evaluation metrics on the testing dataset
mae = mean_absolute_error(y_test, test_predictions)
mse = mean_squared_error(y_test, test_predictions)
r2 = r2_score(y_test, test_predictions)

print("Testing Model MAE:", mae)
print("Testing Model MSE:", mse)
print("Testing Model R2 Score:", r2)

from sklearn.model_selection import GridSearchCV

# Define a parameter grid for hyperparameter tuning
param_grid = {
    'n_estimators': [50, 100, 200],  # Number of trees in the forest
    'max_depth': [3, 5, 7],         # Maximum depth of each tree
    'learning_rate': [0.01, 0.1, 0.2],  # Learning rate
    'min_child_weight': [1, 3, 5],  # Minimum sum of instance weight (hessian) needed in a child
    'subsample': [0.8, 0.9, 1.0],  # Fraction of samples used for training trees
    'colsample_bytree': [0.8, 0.9, 1.0]  # Fraction of features used for training trees
}

# Create the XGBoost regressor
xgb_model = XGBRegressor()

# Initialize GridSearchCV for hyperparameter tuning
grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid,
                           scoring='neg_mean_squared_error', cv=5, verbose=1)

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_

# Use the best model from the grid search
best_model = grid_search.best_estimator_

# Predict with the best model
best_predictions = best_model.predict(X_test)

# Calculate evaluation metrics with the best model
best_mae = mean_absolute_error(y_test, best_predictions)
best_mse = mean_squared_error(y_test, best_predictions)
best_r2 = r2_score(y_test, best_predictions)

print("Best Model MAE:", best_mae)
print("Best Model MSE:", best_mse)
print("Best Model R2 Score:", best_r2)

print("Best Hyperparameters:", best_params)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Assuming you already have a trained model (e.g., best_model from hyperparameter tuning)
# Make predictions on the testing dataset
test_predictions = best_model.predict(X_test)

# Calculate evaluation metrics on the testing dataset
mae = mean_absolute_error(y_test, test_predictions)
mse = mean_squared_error(y_test, test_predictions)
r2 = r2_score(y_test, test_predictions)

print("Testing Model MAE:", mae)
print("Testing Model MSE:", mse)
print("Testing Model R2 Score:", r2)

